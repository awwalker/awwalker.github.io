
<!DOCTYPE HTML>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
    
    <title>Crag Path Visualization</title>
    <link rel="stylesheet" type="text/css" href="../../../dist/semantic.css" />
    <link rel="stylesheet" type="text/css" href="./crags-post.css" />
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/semantic-ui/0.13.0/css/semantic.min.css">
    
    <script src="../../../dist/semantic.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.js"></script>
    
  </head>
  
  <body>
    <div class="index-body">
      <header class="page-header">
        <button class="ui tiny basic left labeled icon button">
          <i class="mini basic left arrow icon"></i>
          Home
        </button>
      </header>
      <header class="main-header no-cover">
        <h1 class="post-title">Visualizing a Path Through US Climbing Areas</h1>
        <section class="post-meta">
          <time class='post-date' datetime="2015-07-28">28 July 2015</time>
        </section>
      </header>
      <section class="post-content">
        <p>The process started with the acquisition of data. In order to code up a visualization I needed
        geographical coordinates in a latitude, longitude format. Before this project I had never used a scraper before.
        Luckily Beautiful Soup 4 <a href="http://www.crummy.com/software/BeautifulSoup/">(BS4)</a> makes scraping incredibly
        easy, even for a first-timer like me. Because I was working with climbing data I decided to get my data from 
        <a href="http://www.mountainproject.com/">Mountain Project</a> (MP). With BS4 I was able to isolate the element on each
        climbing areas page that held the coordinates corresponding to its location, the Python code I used is below. 
        Unfortunately I had to manually tell BS4 which page to visit, however, I definitely saved time pulling directly from
        MP. </p>
        <p>Now that we have our data how can we use it? Originally I simply wanted to know if I could find an efficient route 
        through each of the fifty climbing areas. A simple Google search brought me to 
        <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem">The Traveling Salesman Problem</a> (TSP). The TSP
        is a math problem that simply asks: given a list of cities and the distances between them what is the shortest path
        that visits each city exactly once and returns to the origin.</p>
        <p>The problem is deceptively simple. While easy to talk out the problem is in fact an 
        <a href="https://en.wikipedia.org/wiki/P_versus_NP_problem"> NP-hard</a> problem. While it is possible to find the 
        exact shortest path the algorithms used to perform these calculations are extremely time consuming. The most 
        direct solution would be to simply try all the different permutations (path options) and see which one is the 
        cheapest (shortest). However this approach yields a run time of O(n!), the factorial of the number of cities 
        the salesman wishes to visit. This solution is obviously impractical once the salesman wishes to visit say 15 
        or 20 cities.</p>
        <p>Several algorithms exist to work on a TSP, some being more accurate, others being faster and more efficient. 
        You can see some of these algorithms in action <a href="https://www.youtube.com/watch?v=SC5CX8drAtU">here</a>.
        <a href="https://en.wikipedia.org/wiki/Simulated_annealing">Simulated Annealing</a> (SA) is a practical approach
        to "solving" a TSP. SA offers a way to use probabilities to calculate an approximaion to the global optimum of a 
        given function. The way SA works is similar to the process that gave it its name. Annealing in metallurgy is a 
        technique that involves heating and cooling a material to both increase its size and reduce its defects. </p>
        <p>At the most basic level the SA algorithm starts with a base solution and then at every step it looks a 
        neighboring city of the current one and decides probabilistically whether to take the salesman there or 
        stay put. The algorithm starts at a given temperature and then at each move the temperature is decreased until 
        it reaches a given level (typically 0). Based on the current temperature the algorithm can decide to choose a 
        "worse" solution in the hopes that later on it will be able to turn it into a better one. At high temperatures 
        this probability is high, and likewise at lower temperatures the probability is low. This "cooling" process is 
        what allows SA to provide a global approximation, rather than a local one like some other algorithms do.
        Each run of an SA algorithm provides a new approximation so different routes will be suggested, however, each suggested
        route is by far more efficient than one found through a greedy algorithm.</p>
        <p>Check out the code below to get a better idea of what the Simulated Annealing algorithm is and how to implement it.</p>
        
      </section>
